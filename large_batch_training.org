:PROPERTIES:
:ID:       1c9a7af1-fe4f-49b7-a19b-961bd125cdb8
:END:
#+title: Large Batch Training
* 2 Linked References to "Large Batch Training"

** {{lars_optimizer.org}{LARS Optimizer}}

Layer-wise Adaptive Rate Scaling (LARS) is a {{lars_optimizer.org::8}{Neural Network Optimizer}}. The
technique allows {{lars_optimizer.org::8}{Large Batch Training}} without significant decrease in accuracy
cite:you17_large_batch_train_convol_networ. One of the secondary goals is
{{lars_optimizer.org::8}{Fast Neural Network Training}}.

** {{gpipe.org}{Gpipe}}

In Gpipe, neural networks with sequential layers are partitioned
across accelerators. The pipeline parallelism divides each input
mini-batch into smaller micro-batches, enabling different accelerators
to work on different micro-batches simultaneously. This is especially
useful in {{gpipe.org::16}{Large Batch Training}}.
