:PROPERTIES:
:ID:       92398d83-69a8-4024-be4b-908f2cc72ac1
:END:
#+title: LARS Optimizer

Layer-wise Adaptive Rate Scaling (LARS) is a [[id:1be857e0-0197-44d1-853a-e0e5b74d1b7b][Neural Network Optimizer]]. The
technique allows [[id:1c9a7af1-fe4f-49b7-a19b-961bd125cdb8][Large Batch Training]] without significant decrease in accuracy
cite:you17_large_batch_train_convol_networ. One of the secondary goals is
[[id:b85483b8-9e57-4b6d-babf-5013f99119a0][Fast Neural Network Training]].

* Implementations
- [[https://github.com/noahgolmant/pytorch-lars][pytorch-lars]]

bibliography:biblio.bib
* 1 Linked References to "LARS Optimizer"

** {{chen20_simpl_framew_contr_learn_visual_repres.org}{chen20_simpl_framew_contr_learn_visual_repres: A simple framework for contrastive learning of visual representations}}

*** We do not train the model with a memory bank
Rather than train with a memory bank, they use a large batch size, and the {{chen20_simpl_framew_contr_learn_visual_repres.org::24}{LARS Optimizer}} to stabilize training.
